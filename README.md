### Bayes-PFL ï¼ˆAccepted by CVPR 2025ï¼‰
![](figures/framework.png)

**Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection**

Zhen Qu, Xian Tao, Xinyi Gong, ShiChen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding

[Paper link](https://arxiv.org/pdf/2503.10080)

## Table of Contents
* [ğŸ“– Introduction](#introduction)
* [ğŸ”§ Environments](#environments)
* [ğŸ“Š Data Preparation](#data-preparation)
* [ğŸš€ Run Experiments](#run-experiments)
* [ğŸ”— Citation](#citation)
* [ğŸ™ Acknowledgements](#acknowledgements)
* [ğŸ“œ License](#license)

## Introduction
**This repository contains source code for VCP-CLIP implemented with PyTorch.** 


Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limits generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both imagespecific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and improve the modelâ€™s generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-model attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our methodâ€™s superior performance.


## Environments
Create a new conda environment and install required packages.
```
conda create -n Bayes_PFL python=3.9
conda activate VCP_env
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
```

**Experiments are conducted on a NVIDIA RTX 3090.**


## Data Preparation
 
#### MVTec-AD and VisA 

> **1ã€Download and prepare the original [MVTec-AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) and [VisA](https://amazon-visual-anomaly.s3.us-west-2.amazonaws.com/VisA_20220922.tar) datasets to any desired path. The original dataset format is as follows:**

```
path1
â”œâ”€â”€ mvtec
    â”œâ”€â”€ bottle
        â”œâ”€â”€ train
            â”œâ”€â”€ good
                â”œâ”€â”€ 000.png
        â”œâ”€â”€ test
            â”œâ”€â”€ good
                â”œâ”€â”€ 000.png
            â”œâ”€â”€ anomaly1
                â”œâ”€â”€ 000.png
        â”œâ”€â”€ ground_truth
            â”œâ”€â”€ anomaly1
                â”œâ”€â”€ 000.png
```

```
path2
â”œâ”€â”€ visa
    â”œâ”€â”€ candle
        â”œâ”€â”€ Data
            â”œâ”€â”€ Images
                â”œâ”€â”€ Anomaly
                    â”œâ”€â”€ 000.JPG
                â”œâ”€â”€ Normal
                    â”œâ”€â”€ 0000.JPG
            â”œâ”€â”€ Masks
                â”œâ”€â”€ Anomaly
                    â”œâ”€â”€ 000.png
    â”œâ”€â”€ split_csv
        â”œâ”€â”€ 1cls.csv
        â”œâ”€â”€ 1cls.xlsx
```

> **2ã€Standardize the MVTec-AD and VisA datasets to the same format and generate the corresponding .json files.**

- run **./dataset/make_dataset_new.py** to generate standardized datasets **./dataset/mvisa/data/visa** and **./dataset/mvisa/data/mvtec**
- run **./dataset/make_meta.py** to generate **./dataset/mvisa/data/meta_visa.json** and **./dataset/mvisa/data/meta_mvtec.json** (This step can be skipped since we have already generated them.)

The format of the standardized datasets is as follows:

```
./datasets/mvisa/data
â”œâ”€â”€ visa
    â”œâ”€â”€ candle
        â”œâ”€â”€ train
            â”œâ”€â”€ good
                â”œâ”€â”€ visa_0000_000502.bmp
        â”œâ”€â”€ test
            â”œâ”€â”€ good
                â”œâ”€â”€ visa_0011_000934.bmp
            â”œâ”€â”€ anomaly
                â”œâ”€â”€ visa_000_001000.bmp
        â”œâ”€â”€ ground_truth
            â”œâ”€â”€ anomaly1
                â”œâ”€â”€ visa_000_001000.png
â”œâ”€â”€ mvtec
    â”œâ”€â”€ bottle
        â”œâ”€â”€ train
            â”œâ”€â”€ good
                â”œâ”€â”€ mvtec_000000.bmp
        â”œâ”€â”€ test
            â”œâ”€â”€ good
                â”œâ”€â”€ mvtec_good_000272.bmp
            â”œâ”€â”€ anomaly
                â”œâ”€â”€ mvtec_broken_large_000209.bmp
        â”œâ”€â”€ ground_truth
            â”œâ”€â”€ anomaly
                â”œâ”€â”€ mvtec_broken_large_000209.png

â”œâ”€â”€ meta_mvtec.json
â”œâ”€â”€ meta_visa.json
```

## Run Experiments


## Citation
Please cite the following paper if the code help your project:

```bibtex
@article{qu2025bayesian,
  title={Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection},
  author={Qu, Zhen and Tao, Xian and Gong, Xinyi and Qu, Shichen and Chen, Qiyu and Zhang, Zhengtao and Wang, Xingang and Ding, Guiguang},
  journal={arXiv preprint arXiv:2503.10080},
  year={2025}
```

## Acknowledgements


## License
The code and dataset in this repository are licensed under the [MIT license](https://mit-license.org/).